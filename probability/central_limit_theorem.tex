\section{Central Limit Theorem}

\begin{theorem}
    Let $\{X_{n}\}_{n \in \mathbb{N}}$ be a sequence of independent and identically distributed random variables, all with moment generating functions, such that
    \begin{align*}
        \lim_{n \longrightarrow \infty} M_{X_{n}}(t) \hspace{2pt} = \hspace{2pt} M_{X}(t) \hspace{20pt} \text{for all} \hspace{10pt} t \hspace{10pt} \text{such that} \hspace{4pt} -\delta \hspace{2pt} < \hspace{2pt} t \hspace{2pt} < \hspace{2pt} \delta \hspace{2pt} , \hspace{20pt} \delta \hspace{2pt} > \hspace{2pt} 0
    \end{align*}
    where $M_{X}(t)$ is a moment generating function. There exists a unique $F_{X}$ with moments determined by $M_{X}(t)$, and for all $x$ where $F_{X}(x)$ is continuous, we have
    \begin{align*}
        \lim_{n \longrightarrow \infty} F_{X_{n}}(x) \hspace{2pt} = \hspace{2pt} F_{X}(x)
    \end{align*}
\end{theorem}

\begin{theorem}
    Let $\{X_{i}\}_{i \in \mathbb{N}}$ be a sequence of independent and identically distributed random variables, all with moment generating functions. Let
    \begin{align*}
        E(X_{i}) \hspace{2pt} = \hspace{2pt} \mu \hspace{20pt} \text{and} \hspace{20pt} Var(X_{i}) \hspace{2pt} = \hspace{2pt} \sigma^{2}
    \end{align*}
    Define 
    \begin{align*}
        \overline{X}_{n} \hspace{2pt} = \hspace{2pt} \dfrac{1}{n} \sum_{i = 1}^{n} X_{i} \hspace{20pt} \text{and} \hspace{20pt} G_{n}(x) \hspace{2pt} = \hspace{2pt} \sqrt{n}\dfrac{(\overline{X}_{n} - \mu)}{\sigma}
    \end{align*}
    Then for any $x \in \mathbb{R}$, we have
    \begin{align*}
        \lim_{n \longrightarrow \infty} G_{n}(x) \hspace{2pt} = \hspace{2pt} \int_{-\infty}^{x} \dfrac{1}{\sqrt{2 \pi}} e^{-y^{2}/2} dy
    \end{align*}
    \begin{proof}
        We have that
        \begin{align*}
            \sum_{i \in \mathbb{N}_{n}} Y_{i} \hspace{2pt} &= \hspace{2pt} \dfrac{X_{1} - \mu}{\sigma} + \cdots + \dfrac{X_{n} - \mu}{\sigma} \\[1ex]
            &= \hspace{2pt} \dfrac{1}{\sigma} (X_{1} + \cdots + X_{n} - n\mu) \\[1ex]
            &= \hspace{2pt} \dfrac{n(\overline{X}_{n} - \mu)}{\sigma}
        \end{align*}
        which gives us
        \begin{align*}
            \dfrac{1}{\sqrt{n}} \sum_{i \in \mathbb{N}_{n}} Y_{i} \hspace{2pt} = \hspace{2pt} \dfrac{\sqrt{n}(\overline{X}_{n} - \mu)}{\sigma}
        \end{align*}
        By Theorem \ref{mgf_sum_multiple_random_variables}, first, then by Theorem \ref{mgf_single_random_variable_in_linear_form}, we have the following
        \begin{align*}
            M_{\frac{1}{\sqrt{n}}\sum_{i \in \mathbb{N}_{n}}Y_{i}} (t) \hspace{2pt} &= \hspace{2pt} M_{\frac{1}{\sqrt{n}}Y_{1} + \cdots + \frac{1}{\sqrt{n}}Y_{n}} (t) \\[1ex]
            &= \hspace{2pt} M_{\frac{1}{\sqrt{n}}Y_{1}} (t) \cdots M_{\frac{1}{\sqrt{n}}Y_{n}} (t) \\[1ex]
            &= \hspace{2pt} M_{Y_{1}} \Big(\dfrac{t}{\sqrt{n}}\Big) \cdots M_{Y_{n}} \Big(\dfrac{t}{\sqrt{n}}\Big)
        \end{align*}
        Since $\{Y_{i}\}_{i \in \mathbb{N}_{n}}$ is a set of independent and identically distributed random variables, we may write the following
        \begin{align*}
            Y \sim Y_{i} \hspace{10pt} \forall i \in \mathbb{N}_{n} 
        \end{align*}
        giving us
        \begin{align*}
            M_{\frac{1}{\sqrt{n}}\sum_{i \in \mathbb{N}_{n}}Y_{i}} (t) \hspace{2pt} = \hspace{2pt} M_{Y_{1}} \Big(\dfrac{t}{\sqrt{n}}\Big) \cdots M_{Y_{n}} \Big(\dfrac{t}{\sqrt{n}}\Big) \hspace{2pt} = \hspace{2pt} \Big(M_{Y} \Big(\dfrac{t}{\sqrt{n}}\Big) \Big)^{n}
        \end{align*}
        We can expand $M_{Y} \Big(\dfrac{t}{\sqrt{n}}\Big)$ to a Taylor Series, as follows
        \begin{align*}
            M_{Y} \Big(\dfrac{t}{\sqrt{n}}\Big) \hspace{2pt} = \hspace{2pt} \sum_{k \in \mathbb{N}} M_{Y}^{(k)} (0) \Big(\dfrac{t}{\sqrt{n}} - 0\Big)^{k}
        \end{align*}
        We have that 
        \begin{align*}
            M_{Y}^{(0)} (0) \hspace{2pt} = \hspace{2pt} 1
        \end{align*}
        since $M_{Y}^{(0)} (0) \hspace{2pt} = \hspace{2pt} F_{Y} (\infty) \hspace{2pt} = \hspace{2pt} P_{Y} (Y \hspace{2pt} < \hspace{2pt} \infty)$. \\[1ex]
        The mean of random variable $Y \hspace{2pt} = \hspace{2pt} \Big(\dfrac{X - \mu}{\sigma}\Big)$ is $0$, which we can also observe via the following
        \begin{align*}
            M_{Y}^{(1)} (0) \hspace{2pt} = \hspace{2pt} 0
        \end{align*}
        This is true, since for any random variable, $V$, with mean $\mu$ and variance $\sigma^{2}$, we have
        \begin{align*}
            E\Big(\dfrac{V - \mu}{\sigma}\Big) \hspace{2pt} = \hspace{2pt} \dfrac{1}{\sigma} (E(V) - \mu) \hspace{2pt} = \hspace{2pt} 0 
        \end{align*}
        Finally, the variance of random variable $Y \hspace{2pt} = \hspace{2pt} \Big(\dfrac{X - \mu}{\sigma}\Big)$ is $1$, which we can also observe via the following
        \begin{align*}
            M_{Y}^{(2)} (0) \hspace{2pt} = \hspace{2pt} 1
        \end{align*}
        This is true, since for any random variable, $V$, with mean $\mu$ and variance $\sigma^{2}$, we have
        \begin{align*}
            E\Big(\Big(\dfrac{V - \mu}{\sigma}\Big)^{2}\Big) \hspace{2pt} &= \hspace{2pt} E\Big(\dfrac{1}{\sigma^{2}}(V^{2} - 2\mu V + \mu^{2})\Big) \\[1ex]
            &= \hspace{2pt} \dfrac{1}{\sigma^{2}} (E(V^{2}) - 2\mu^{2} + \mu^{2}) \\[1ex]
            &= \hspace{2pt} \dfrac{1}{\sigma^{2}} (E(V^{2}) - \mu^{2}) \\[1ex]
            &= \hspace{2pt} \dfrac{1}{\sigma^{2}} (\sigma^{2}) \hspace{2pt} = \hspace{2pt} 1
        \end{align*}
    \end{proof}
\end{theorem}