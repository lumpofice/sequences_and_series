\section{Probability}

\begin{definition}
Let $X$ be a function from space $S$ to the real numbers that follows the mapping
\begin{align*}
    X(s) \hspace{2pt} = \hspace{2pt} x \hspace{20pt} \text{where} \hspace{20pt} s \in S \hspace{10pt} \text{and} \hspace{10pt} x \in \mathbb{R}
\end{align*}
$X$ is referred to as a random variable.
\label{random_variable}
\end{definition}

\begin{definition}
The cumulative distribution function of a function $X$, as described in Definition \ref{random_variable}, denoted by $F_{X}(x)$ is defined as
\begin{align*}
    F_{X}(x) \hspace{2pt} = \hspace{2pt} P_{X}(X \hspace{2pt} \leq \hspace{2pt} x) \hspace{20pt} \text{for all} \hspace{4pt} x \in \text{Rng}(X)
\end{align*}
\label{cumulative_distribution_function}
\end{definition}

\begin{theorem}
The function $F_{X}$ is a cumulative distribution function if and only if it meets the following three conditions:
\begin{align*}
    &\text{i)} \hspace{10pt} \lim_{x \longrightarrow -\infty} F_{X}(x) \hspace{2pt} = \hspace{2pt} 0 \hspace{10pt} \text{and} \hspace{10pt} \lim_{x \longrightarrow \infty} F_{X}(x) \hspace{2pt} = \hspace{2pt} 1\\[2ex]
    &\text{ii)} \hspace{10pt} F_{X} \hspace{4pt} \text{is a nondecreasing function of} \hspace{4pt} x\\[2ex]
    &\text{iii)} \hspace{10pt} F_{X} \hspace{4pt} \text{is right-continuous. Meaning, for each} \hspace{4pt} x_{0} \in \text{Rng}(X) \hspace{4pt} \text{we have} \hspace{10pt} \lim_{x \longrightarrow x_{0}^{+}} F_{X}(x) = F_{X}(x_{0})
\end{align*}
\end{theorem}

\begin{definition}
A function $X$, as described in Definition \ref{random_variable}, is continuous if $F_{X}$ is a continuous function of $x$.
\end{definition}

\begin{definition}
A function $X$, as described in Definition \ref{random_variable}, is discrete if $F_{X}$ is a step function of $x$.
\end{definition}

\begin{definition}
The probability mass function of a discrete function $X$, as described in Definition \ref{random_variable}, is given by 
\begin{align*}
    f_{X}(x) \hspace{2pt} = \hspace{2pt} P_{X}(x) \hspace{20pt} \text{for all} \hspace{4pt} x \in \text{Rng}(X)
\end{align*}
\label{probability_mass_function}
\end{definition}

\begin{note}
Connecting Definition \ref{cumulative_distribution_function} with Definition \ref{probability_mass_function}, we have
\begin{align*}
    F_{X}(x) \hspace{2pt} = \hspace{2pt} P_{X}(X \hspace{2pt} \leq \hspace{2pt} x) \hspace{2pt} = \hspace{2pt} \sum_{i \hspace{2pt} \leq \hspace{2pt} x} P_{X}(X \hspace{2pt} = \hspace{2pt} i) \hspace{2pt} = \hspace{2pt} \sum_{i \hspace{2pt} \leq \hspace{2pt} x} f_{X}(i)
\end{align*}
\end{note}

\begin{definition}
The probability density function, $f_{X}$, of a continuous function $X$, as described in Definition \ref{random_variable}, is the function that satisfies 
\begin{align*}
    F_{X}(x) \hspace{2pt} = \hspace{2pt} \int_{-\infty}^{x} f_{X}(t) dt \hspace{10pt} \text{for all} \hspace{4pt} x \in \text{Rng}(X)
\end{align*}
\end{definition}

\begin{theorem}
A function $f_{X}$ is a probability mass function (or a probability density function) of a random variable $X$, as describe in Definition \ref{random_variable}, if and only if
\begin{align*}
    &i) \hspace{10pt} f_{X}(x) \hspace{2pt} \geq \hspace{2pt} 0 \hspace{10pt} \text{for all} \hspace{4pt} x \in \text{Rng}(X)\\[2ex]
    &ii)_\text{(discrete)} \hspace{10pt} \sum_{x \in Rng(X)} f_{X}(x) \hspace{2pt} = \hspace{2pt} 1\\[2ex]
    &ii)_\text{(continuous)} \hspace{10pt} \int_{-\infty}^{\infty} f_{X}(x) dx \hspace{2pt} = \hspace{2pt} 1
\end{align*}
\end{theorem}

\begin{definition}
    The expected value of a random variable $g(X)$ follows the form
    \begin{align*}
        &i)_\text{($X$ is discrete)} \hspace{10pt} E(g(X)) \hspace{2pt} = \hspace{2pt} \sum_{x \in Rng(X)} g(x)f_{X}(x) \\[2ex]
        &ii)_\text{($X$ is continuous)} \hspace{10pt} E(g(X)) \hspace{2pt} = \hspace{2pt} \int_{-\infty}^{\infty} g(x)f_{X}(x)dx
    \end{align*}
    assuming $E(g(X)) \hspace{2pt} < \hspace{2pt} \infty$
\end{definition}

The expected value of a random variable, $X$, is also known as the expectation of random variable, $X$.

\begin{theorem}
    \label{expectation_linear_combination}
    Let $g_{1}(X)$ and $g_{2}(X)$ be random variables with finite expectations, and let $a$, $b$, and $c$ be members of $\mathbb{R}$. Then we have
    \begin{align*}
        E(ag_{1}(X) + bg_{2}(X) + c) \hspace{2pt} = \hspace{2pt} aE(g_{1}(X)) + bE(g_{2}(X)) + c
    \end{align*}
\end{theorem}

\begin{definition}
    \label{nth_moment}
    The $n^{\text{th}}$ moment of a random variable, $X$, is defined as
    \begin{align*}
        E(X^{n})
    \end{align*}
\end{definition}

\begin{definition}
    The $n^{\text{th}}$ central moment of a random variable, $X$, is defined as
    \begin{align*}
        E((X - E(X))^{n})
    \end{align*}
\end{definition}

\begin{definition}
    The variance of a random variable, $X$, is the second central moment of $X$
    \begin{align*}
        Var(X) \hspace{2pt} = \hspace{2pt} E((X - E(X))^{2}) 
    \end{align*}
\end{definition}

We can expand this definition of variance, and in using Theorem \ref{expectation_linear_combination}, recalling that the expected value of random variable, $X$, is a member of $\mathbb{R}$, we have
\begin{align*}
    Var(X) \hspace{2pt} &= \hspace{2pt} E((X - E(X))^{2}) \\[1ex]
    &= \hspace{2pt} E(X^{2} - 2XE(X) + (E(X))^{2}) \\[1ex]
    &= \hspace{2pt} E(X^{2}) - E(2XE(X)) + E((E(X))^{2}) \\[1ex]
    &= \hspace{2pt} E(X^{2}) - 2(E(X))^{2} + (E(X))^{2} \\[1ex]
    &= \hspace{2pt} E(X^{2}) - (E(X))^{2}
\end{align*}
arriving at an alternative way of writing the variance of random variable, $X$.

\begin{theorem}
    If random variable, $X$, has a finite variance, then for any $a$ and $b$ in $\mathbb{R}$, we have
    \begin{align*}
        Var(aX + b) \hspace{2pt} = \hspace{2pt} a^{2}Var(X)
    \end{align*}
\end{theorem}

\begin{definition}
    The moment generating function of random variable, $X$, follows the form
    \begin{align*}
        &i)_\text{($X$ is discrete)} \hspace{10pt} M_{X}(t) \hspace{2pt} = \hspace{2pt} E(e^{tX}) \hspace{2pt} = \hspace{2pt} \sum_{x \in Rng(X)} e^{tX}f_{X}(x) \\[2ex]
        &ii)_\text{($X$ is continuous)} \hspace{10pt} M_{X}(t) \hspace{2pt} = \hspace{2pt} E(e^{tX}) \hspace{2pt} = \hspace{2pt} \int_{-\infty}^{\infty} e^{tX}f_{X}(x)dx
    \end{align*}
assuming that $M_{X}(t)$ exists for any $t$ belonging to some neighborhood of $0$.
\end{definition}

\begin{theorem}
    \label{nth_moment_by_derivative_of_mgf}
    If the moment generating function for random variable, $X$, exists, then
    \begin{align*}
        E(X^{n}) \hspace{2pt} = \hspace{2pt} M_{X}^{(n)}(0) \hspace{2pt} = \hspace{2pt} \dfrac{d^{n}}{dt^{n}}M_{X}(t)\Big|_{t = 0}  
    \end{align*}
\end{theorem}

So, we see by Definition \ref{nth_moment} and Theorem \ref{nth_moment_by_derivative_of_mgf} that the $n^{\text{th}}$ moment of random variable, $X$, is the $n^{\text{th}}$ derivative of the moment generating function of random variable, $X$, evaluated at $t \hspace{2pt} = \hspace{2pt} 0$.

\begin{definition}
    A random vector is an $n$-dimensional function from space $S$ to space $\mathbb{R}^{n}$ following the mapping
    \begin{align*}
        s \hspace{2pt} \mapsto \hspace{2pt} (X_{1}(s), X_{2}(s), \dots , X_{n}(s)) \hspace{20pt} \text{where} \hspace{20pt} s \in S \hspace{20pt} \text{and} \hspace{20pt} n \in \mathbb{N}
    \end{align*}
\end{definition}

\begin{definition}
    Let $(X_{1}, X_{2})$ be a discrete bivariate random vector. Then the joint probability mass function, from $\mathbb{R}^{2}$ to $\mathbb{R}$, of random vector $(X_{1}, X_{2})$ is 
    \begin{align*}
        f_{X_{1}, X_{2}}(x_{1}, x_{2}) \hspace{2pt} = \hspace{2pt} P_{X_{1}, X_{2}}(X_{1} = x_{1}, X_{2} = x_{2})
    \end{align*}
\end{definition}

\begin{definition}
    Let $(X_{1}, X_{2})$ be a continuous random vector. Then the joint probability density function, $f_{X_{1}, X_{2}}(x_{1}, x_{2})$, is the function satisfying the following
    \begin{align*}
        P_{X_{1}, X_{2}}(X_{1} \hspace{2pt} \leq \hspace{2pt} x_{1}, X_{2} \hspace{2pt} \leq \hspace{2pt} x_{2}) \hspace{2pt} &= \hspace{2pt} F_{X_{1}, X_{2}}(x_{1}, x_{2}) \\[1ex]
        &= \hspace{2pt} \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X_{1}, X_{2}}(s,t) dt ds \hspace{10pt} \forall (x_{1}, x_{2}) \in A \hspace{10pt} \forall A \subset \mathbb{R}^{2}
    \end{align*}
\end{definition}

\begin{theorem}
    Let $(X_{1}, X_{2})$ be a bivariate random vector with joint probability mass function $f_{X_{1}, X_{2}}$. Then the marginal probability mass functions of $X_{1}$ and $X_{2}$ are given by
    \begin{align*}
        &P_{X_{1}}(X_{1} \hspace{2pt} = \hspace{2pt} x_{1}) \hspace{2pt} = \hspace{2pt} f_{X_{1}}(x_{1}) \hspace{2pt} = \hspace{2pt} \sum_{x_{2} \in \mathbb{R}} f_{X_{1}, X_{2}}(x_{1}, x_{2}) \\[1ex]
        &P_{X_{2}}(X_{2} \hspace{2pt} = \hspace{2pt} x_{2}) \hspace{2pt} = \hspace{2pt} f_{X_{2}}(x_{2}) \hspace{2pt} = \hspace{2pt} \sum_{x_{1} \in \mathbb{R}} f_{X_{1}, X_{2}}(x_{1}, x_{2})
    \end{align*}
\end{theorem}

\begin{theorem}
    Let $(X_{1}, X_{2})$ be a bivariate random vector with joint probability density function $f_{X_{1}, X_{2}}$. Then the marginal probability density functions of $X_{1}$ and $X_{2}$ are given by
    \begin{align*}
        &P_{X_{1}}(X_{1} \hspace{2pt} = \hspace{2pt} x_{1}) \hspace{2pt} = \hspace{2pt} f_{X_{1}}(x_{1}) \hspace{2pt} = \hspace{2pt} \int_{-\infty}^{\infty} f_{X_{1}, X_{2}}(x_{1}, x_{2}) dx_{2} \\[1ex]
        &P_{X_{2}}(X_{2} \hspace{2pt} = \hspace{2pt} x_{2}) \hspace{2pt} = \hspace{2pt} f_{X_{2}}(x_{2}) \hspace{2pt} = \hspace{2pt} \int_{-\infty}^{\infty} f_{X_{1}, X_{2}}(x_{1}, x_{2}) dx_{1}
    \end{align*}
\end{theorem}

\begin{definition}
    Let $(X_{1}, X_{2})$ be a random vector. Then the random variables, $X_{1}$, $X_{2}$, for their respective marginal probability density functions or probability mass functions, $f_{X_{1}}$, $f_{X_{2}}$, are independent, if for every $x \in \mathbb{R}$ and for every $y \in \mathbb{R}$
    \begin{align*}
        f_{X{1}, X{2}}(x_{1}, x_{2}) \hspace{2pt} = \hspace{2pt} f_{X_{1}}(x_{1}) f_{X_{2}}(x_{2})
    \end{align*}
\end{definition}

\begin{theorem}
    Let $(X_{1}, X_{2})$ be a random vector. Then the random variables, ${X_{1}}$, ${X_{2}}$, are independent if and only if there exist functions, $g(X_{1})$, $h(X_{2})$, such that, for the probability density function or probability mass function, $f_{X_{1}, X_{2}}$, we have for any $x_{1} \in \mathbb{R}$ and for any $x_{2} \in \mathbb{R}$
    \begin{align*}
        f_{X_{1}, X_{2}}(x_{1}, x_{2}) \hspace{2pt} = \hspace{2pt} g(x_{1}) h(x_{2})
    \end{align*}
\end{theorem}

\begin{theorem}
    Let $X_{1}$ and $X_{2}$ be independent random variables. Let $g$ be a function only of $X_{1}$. Let $h$ be a function only $X_{2}$. Then
    \begin{align*}
        E(g(X_{1}) h(X_{2})) \hspace{2pt} = \hspace{2pt} E(g(X_{1})) E(h(X_{2}))
    \end{align*}
\end{theorem}

\begin{theorem}
    Let $X_{1}$ and $X_{2}$ be independent random variables, and let $M_{X_{1}}(t)$ and $M_{X_{2}}(t)$ be their respective, existing moment generating functions. Let there be a random variable, $Z$, such that
    \begin{align*}
        Z \hspace{2pt} = \hspace{2pt} X_{1} + X_{2}
    \end{align*}
    Then we have
    \begin{align*}
        M_{Z}(t) \hspace{2pt} = \hspace{2pt} M_{X_{1}}(t) M_{X_{2}}(t)
    \end{align*}
\end{theorem}

\begin{definition}
    \label{iid}
    Let $\vec{X}_{1}$, $\dots$, $\vec{X}_{n}$ be random vectors, and let $f_{\vec{X}_{1}, \dots , \vec{X}_{n}}$ be their joint probability density function or the joint probability mass function. Let $f_{\vec{X}_{i}}(\vec{x}_{i})$ be the marginal probability density function or the marginal probability mass function for random vector, $\vec{X}_{i}$. Then random vectors, $\vec{X}_{1}$, $\dots$, $\vec{X}_{n}$, are mutually independent if, for all $(\vec{x}_{1}, \dots , \vec{x}_{n}) \in Dom(f_{\vec{X}_{1}, \dots , \vec{X}_{n}})$ , we have
    \begin{align*}
        f_{\vec{X}_{1}, \dots , \vec{X}_{n}}(\vec{x}_{1}, \dots , \vec{x}_{n}) \hspace{2pt} = \hspace{2pt} \prod_{i = 1}^{n} f_{\vec{X}_{i}}(\vec{x}_{i})
    \end{align*}
\end{definition}

The random variables in Definition \ref{iid} are known as independent and identically distributed random variables.

\begin{theorem}
    Let $X_{1}$, $\dots$, $X_{n}$ be mutually independent random variables, and let $M_{X_{1}}(t)$, $\dots$, $M_{X_{n}}(t)$ be their respective, existing moment generating functions. Let there be a random variable, $Z$, such that
    \begin{align*}
        Z \hspace{2pt} = \hspace{2pt} X_{1} + \dots + X_{n}
    \end{align*}
    Then we have
    \begin{align*}
        M_{Z}(t) \hspace{2pt} = \hspace{2pt} M_{X_{1}}(t) \cdots M_{X_{n}}(t)
    \end{align*}
\end{theorem}

\begin{theorem}
    For any constants $a$ and $b$, the moment generating function of a random variable following the form
    \begin{align*}
        aX + b
    \end{align*}
    is given by
    \begin{align*}
        M_{aX + b}(t) \hspace{2pt} = \hspace{2pt} e^{bt} M_{X}(at)
    \end{align*}
    \begin{proof}
        We have
        \begin{align*}
            M_{aX + b} \hspace{2pt} &= \hspace{2pt} E(e^{(aX + b)t}) \\[1ex]
            &= \hspace{2pt} E(e^{(aX)t} e^{bt}) \\[1ex]
            &= \hspace{2pt} e^{bt} E(e^{(aX)t}) \\[1ex]
            &= \hspace{2pt} e^{bt} M_{X}(at)
        \end{align*}
    \end{proof}
\end{theorem}