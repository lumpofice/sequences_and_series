\section{Power Series}

\begin{definition}
Given a sequence $\{c_{n}\}_{n=0}^{\infty}$ of real numbers and
\begin{align*}
    f(x) = \sum_{n=0}^{\infty} c_{n} (x - a)^{n}
\end{align*}
where $a$ is a real number, we say $f$ is a power series centered at $a$.
\end{definition}

\begin{note}
The reader may replace the word ``radius" in this section with the word ``interval", but in general, ``radius" means ``ball", whether that ``ball" be a one-dimensional interval, a two-dimensional disk, a three-dimensional sphere, etc. Thus, for any natural number $n$, one may refer to a ``radius" in $n$-dimensional space as a ``ball".
\end{note}

Function $f$ will have a radius of convergence for some $x$, since
\begin{align*}
    f(x) &= c_{0} (x - a)^{0} + c_{1} (x - a)^{1} + c_{2} (x - a)^{2} + \cdots\\[2ex]
    &= c_{0} + c_{1} (x - a) + c_{2} (x - a)^{2} + \cdots
\end{align*}
which becomes the finite number, $c_{0}$, as soon as $x = a$. Our job is to find this radius of convergence and the object to which the series converges. 

\begin{note}
A series converges if its sequence of partial sums converges.
\end{note}

We begin with finding the radius of convergence, for which there are three possible outcomes
\begin{itemize}
    \item $f$ converges for $x \in \{a\}$
    \item $f$ converges for $x \in (-\infty, \infty)$
    \item $f$ converges for $x \in (-R, R)$, where $R$ is finite. 
\end{itemize}

\begin{example}
Take the function
\begin{align*}
    f(x) = \sum_{n=0}^{\infty} x^{n} = 1 + x + x^{2} + x^{3} + \cdots = \dfrac{1}{1-x}
\end{align*}
which we discovered is convergent when $x \in (-1, 1)$. Function $f$ is the power series centered about $a=0$. Using the ratio test
\begin{align*}
    \Big\lvert \dfrac{x^{n+1}}{x^{n}} \Big\rvert = \lvert x \rvert \hspace{20pt} \text{and} \hspace{20pt} \lim_{n \longrightarrow \infty} \lvert x \rvert = \lvert x \rvert < 1 \hspace{20pt} \text{if and only if} \hspace{20pt} -1 < x < 1
\end{align*}
\end{example}

\begin{theorem}
Let $n$ be a natural number on $[\alpha, \beta]$, let $f$ be a function such that $f$ and all of its derivatives are continuous on $[\alpha, \beta]$ and $f^{(n+1)}$ exists on $(\alpha, \beta)$. If $a$ is in $[\alpha, \beta]$, then for any $x$ in $[\alpha, \beta]$, there exists a point $c$ between $x$ and $a$ such that
\begin{align*}
    f(x) = f(a) + f^{(1)}(a)(x-a) + \dfrac{f^{(2)}(a)}{2!}(x-a)^{2} + \cdots + \dfrac{f^{(n)}(a)}{n!}(x-a)^{n} + \dfrac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}
\end{align*}
where
\begin{align*}
    f(a) + f^{(1)}(a)(x-a) + \dfrac{f^{(2)}(a)}{2!}(x-a)^{2} + \cdots + \dfrac{f^{(n)}(a)}{n!}(x-a)^{n}
\end{align*}
is the $n^{th}$ term in the sequence
\begin{align*}
    \Big\{\sum_{i=0}^{k} \dfrac{f^{(i)}(x_{0})}{i!}(x-a)^{i}\Big\}_{k=0}^{\infty}
\end{align*}
which we may abbreviate as $T_{n}(x)$, and 
\begin{align*}
    \dfrac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}
\end{align*}
is referred to as the remainder, which we may abbreviate as $R_{n}(x)$.
\label{taylors_theorem}
\end{theorem}


We refer to $f$ as the Taylor Series and can write it as
\begin{align*}
    f(x) = \sum_{n=0}^{\infty} \dfrac{f^{(n)}(a)}{n!}(x-a)^{n} \hspace{20pt} \text{for} \hspace{4pt} x \in (-R + a, R + a)
\end{align*}
if and only if
\begin{align*}
    \lim_{n \longrightarrow \infty} R_{n}(x) = 0 \hspace{20pt} \text{for each} \hspace{4pt} x \in (-R + a, R + a)
\end{align*}
We may obtain this limit if there exists a bound on $f^{(n)}(a)$ in a sufficiently small enough window of $a$. Using the limit
\begin{align*}
    \lim_{n \longrightarrow \infty} \dfrac{x^{n}}{n!} = 0
\end{align*}
we have the following
\begin{align*}
    &\text{If} \hspace{10pt} \lvert f^{(n+1)}(x) \rvert \leq M \hspace{10pt} \text{on} \hspace{10pt} \lvert x - a \rvert \leq \delta \\[2ex]
    &\text{then} \hspace{10pt} \lvert R_{n}(x) \rvert \leq \dfrac{M}{(n+1)!} \lvert x - a \rvert^{n+1} \hspace{10pt} \text{on} \hspace{10pt} \lvert x - a \rvert \leq \delta
\end{align*}

\begin{note}
Each $T_{n}(x)$ is a partial sum and is referred to as the $n^{th}$ Taylor polynomial.
\end{note}

\begin{note}
From the definition of a power series to the Taylor polynomial, we see
\begin{align*}
    c_{n} = \dfrac{f^{(n)}(a)}{n!}
\end{align*}
\end{note}

\begin{example}
The Taylor series for $f(x) = e^{x}$ centered at $0$ can be found as follows. We have the general setup
\begin{align*}
    e^{x} = f(x) = c_{0} + c_{1} (x - a) + c_{2} (x - a)^{2} + c_{3} (x - a)^{3} + c_{4} (x - a)^{4} + \cdots 
\end{align*}
Knowing that $a = 0$,
\begin{align*}
    e^{x} = f(x) = c_{0} + c_{1}x + c_{2}x^{2} + c_{3}x^{3} + c_{4}x^{4} + \cdots 
\end{align*}
We may use Theorem \ref{taylors_theorem} to write out the terms of this series, or we may evaluate each derivative at $0$ to establish a pattern
\begin{align*}
    &1 = e^{0} = f(0) = c_{0} + 0 + 0 + \cdots = c_{0} \hspace{20pt} \Longleftrightarrow \hspace{20pt} c_{0} = 1\\[2ex]
    &1 = e^{0} = f^{(1)}(0) = 1 \cdot c_{1} + 0 + 0 + \cdots = 1!c_{1} \hspace{20pt} \Longleftrightarrow \hspace{20pt} c_{1} = \dfrac{1}{1!}\\[2ex]
    &1 = e^{0} = f^{(2)}(0) = 1 \cdot 2 \cdot c_{2} + 0 + 0 + \cdots = 2!c_{2} \hspace{20pt} \Longleftrightarrow \hspace{20pt} c_{2} = \dfrac{1}{2!}\\[2ex]
    &1 = e^{0} = f(0) = 1 \cdot 2 \cdot 3 \cdot c_{3} + 0 + 0 + \cdots = 3!c_{3} \hspace{20pt} \Longleftrightarrow \hspace{20pt} c_{3} = \dfrac{1}{3!}\\[1ex]
    &\cdots\\[1ex]
    &\cdots\\[1ex]
    &\cdots
\end{align*}
Now we may rewrite the series
\begin{align*}
    e^{x} = f(x) &= 1 + \dfrac{1}{1!}x + \dfrac{1}{2!}x^{2} + \dfrac{1}{3!}x^{3} + \dfrac{1}{4!}x^{4} + \cdots = \sum_{n=0}^{\infty} \dfrac{1}{n!}x^{n}
\end{align*}
We prove that $e^{x}$ may be represented by this Taylor series
\end{example}